{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d5b846e-137a-4452-9f67-8ea4dd6e051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c19a96e3-585d-4792-abc7-52c2f8d779e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# from pytorch_fid import fid_score\n",
    "# import lpips\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc09638-dda6-4074-a462-d80903d11a95",
   "metadata": {},
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb612dc-65cf-4170-b1bd-2dc52240fb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DModel(\n",
       "  (conv_in): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): Linear(in_features=32, out_features=128, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): AttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Attention(\n",
       "          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): AttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Attention(\n",
       "          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): AttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Attention(\n",
       "          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): AttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Attention(\n",
       "          (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "          (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 96, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "          (norm2): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2D(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Attention(\n",
       "        (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (to_q): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (to_k): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (to_v): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (norm2): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "\n",
    "# 实例化模型\n",
    "net = UNet2DModel(\n",
    "    sample_size=128,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    block_out_channels=(32, 64, 64),\n",
    "    down_block_types=(\"DownBlock2D\", \"AttnDownBlock2D\", \"AttnDownBlock2D\"),\n",
    "    up_block_types=(\"AttnUpBlock2D\", \"AttnUpBlock2D\", \"UpBlock2D\"),\n",
    ").to(device)\n",
    "\n",
    "# 加载checkpoints\n",
    "checkpoint = torch.load('checkpoints_0708/best_model.pth', map_location=device)\n",
    "# print(checkpoint.keys())\n",
    "\n",
    "# 切换到eval\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "803a57d0-d40c-4e8a-9111-007a6034e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# 测试加载\n",
    "dummy_input = torch.randn(36, 1, 128, 128).to(device)\n",
    "with torch.no_grad():\n",
    "    output = net(dummy_input, timestep=0).sample\n",
    "print(output.shape)  # [36, 1, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232bfcf6-d58b-42dc-b279-8246b78686db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 创建 scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92dbd5f9-6749-45e4-97c6-a502f155145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 换scheduler\n",
    "# 替换为 DDIMScheduler 或 DPMSolverMultistepScheduler\n",
    "from diffusers import DDIMScheduler, DPMSolverMultistepScheduler\n",
    "noise_scheduler_ddim = DDIMScheduler.from_config(noise_scheduler.config) # 50步\n",
    "noise_scheduler_dpm = DPMSolverMultistepScheduler.from_config(noise_scheduler.config) #20-30步\n",
    "noise_scheduler_ddim.set_timesteps(50)\n",
    "noise_scheduler_dpm.set_timesteps(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d046504d-0cd0-4ade-9ef6-f357c262dedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493a5af0f28e4f679232c3b4a41e56e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "class_embedding needs to be initialized in order to use class conditioning",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tqdm(noise_scheduler_dpm\u001b[38;5;241m.\u001b[39mtimesteps):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 10\u001b[0m         residual \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 条件在 class label\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# print(residual.shape)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m noise_scheduler_dpm\u001b[38;5;241m.\u001b[39mstep(residual, t, x)\u001b[38;5;241m.\u001b[39mprev_sample\n",
      "File \u001b[0;32m~/anaconda3/envs/brwtorchtest/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/brwtorchtest/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/brwtorchtest/lib/python3.10/site-packages/diffusers/models/unets/unet_2d.py:306\u001b[0m, in \u001b[0;36mUNet2DModel.forward\u001b[0;34m(self, sample, timestep, class_labels, return_dict)\u001b[0m\n\u001b[1;32m    304\u001b[0m     emb \u001b[38;5;241m=\u001b[39m emb \u001b[38;5;241m+\u001b[39m class_emb\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m class_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_embedding needs to be initialized in order to use class conditioning\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# 2. pre-process\u001b[39;00m\n\u001b[1;32m    309\u001b[0m skip_sample \u001b[38;5;241m=\u001b[39m sample\n",
      "\u001b[0;31mValueError\u001b[0m: class_embedding needs to be initialized in order to use class conditioning"
     ]
    }
   ],
   "source": [
    "# Prepare random x to start from, plus desired labels y\n",
    "num_classes = 36\n",
    "samples_per_class = 8  # 每类生成8张\n",
    "x = torch.randn(num_classes * samples_per_class, 1, 128, 128).to(device)\n",
    "y = torch.tensor([[i] * samples_per_class for i in range(num_classes)]).flatten().to(device)\n",
    "\n",
    "# Reverse diffusion (sampling loop)\n",
    "for t in tqdm(noise_scheduler_dpm.timesteps):\n",
    "    with torch.no_grad():\n",
    "        residual = net(x, t, y)  # 条件在 class label\n",
    "        # print(residual.shape)\n",
    "    x = noise_scheduler_dpm.step(residual, t, x).prev_sample\n",
    "\n",
    "# Postprocess to [0, 1]\n",
    "samples = (x.detach().cpu().clip(-1, 1) + 1) / 2\n",
    "\n",
    "# Show grid of generated images\n",
    "grid = torchvision.utils.make_grid(samples, nrow=samples_per_class)\n",
    "plt.figure(figsize=(15, 40))\n",
    "plt.imshow(grid.permute(1, 2, 0).squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Generated Samples (Conditioned on Class Labels)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1bef7-5031-43dd-a7de-b0d2b1921214",
   "metadata": {},
   "source": [
    "## 1.分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59131c99-c390-4ef0-9ad6-8a1e8afdda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 & 加载分类器\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "resnet50 = models.resnet50(pretrained=True).eval()\n",
    "\n",
    "# LPIPS 模型\n",
    "lpips_fn = lpips.LPIPS(net='alex').eval()\n",
    "\n",
    "def get_predicted_class(img_pil):\n",
    "    \"\"\"用 pretrained 预测图片类别\"\"\"\n",
    "    input_tensor = preprocess(img_pil).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = resnet50(input_tensor)\n",
    "    return output.argmax(dim=1).item()\n",
    "\n",
    "def compute_metrics(original_images, generated_images, target_classes, class_labels):\n",
    "    \"\"\"\n",
    "    original_images, generated_images: list of PIL.Image\n",
    "    target_classes: list of int (target class index)\n",
    "    class_labels: dict {class_index: class_name}\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    ssim_scores, psnr_scores, lpips_scores = [], [], []\n",
    "    \n",
    "    for orig, gen, target in zip(original_images, generated_images, target_classes):\n",
    "        # 分类器评估\n",
    "        pred_class = get_predicted_class(gen)\n",
    "        if pred_class == target:\n",
    "            correct += 1\n",
    "        \n",
    "        # SSIM\n",
    "        ssim_val = ssim(np.array(orig.convert('L')), np.array(gen.convert('L')))\n",
    "        ssim_scores.append(ssim_val)\n",
    "        \n",
    "        # PSNR\n",
    "        mse = np.mean((np.array(orig) - np.array(gen)) ** 2)\n",
    "        psnr_val = 20 * np.log10(255.0 / np.sqrt(mse)) if mse > 0 else 100\n",
    "        psnr_scores.append(psnr_val)\n",
    "        \n",
    "        # LPIPS\n",
    "        lpips_val = lpips_fn(preprocess(orig).unsqueeze(0), preprocess(gen).unsqueeze(0))\n",
    "        lpips_scores.append(lpips_val.item())\n",
    "        \n",
    "        # 可视化\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "        axes[0].imshow(orig)\n",
    "        axes[0].set_title(f\"Original: {class_labels[target]}\")\n",
    "        axes[1].imshow(gen)\n",
    "        axes[1].set_title(f\"Generated (Pred: {class_labels[pred_class]})\")\n",
    "        diff = np.abs(np.array(orig).astype(float) - np.array(gen).astype(float))\n",
    "        axes[2].imshow(diff.astype(np.uint8))\n",
    "        axes[2].set_title(\"Difference Map\")\n",
    "        for ax in axes: ax.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    accuracy = correct / len(generated_images)\n",
    "    print(f\"\\n Class-conditioned Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"Avg SSIM: {np.mean(ssim_scores):.4f}\")\n",
    "    print(f\"Avg PSNR: {np.mean(psnr_scores):.2f} dB\")\n",
    "    print(f\"Avg LPIPS: {np.mean(lpips_scores):.4f}\")\n",
    "    \n",
    "    # 可选: FID (需要把图片存到目录)\n",
    "    # fid_value = fid_score.calculate_fid_given_paths([gen_dir, orig_dir], batch_size=50, device='cuda', dims=2048)\n",
    "    # print(f\"FID: {fid_value:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"ssim\": np.mean(ssim_scores),\n",
    "        \"psnr\": np.mean(psnr_scores),\n",
    "        \"lpips\": np.mean(lpips_scores)\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
